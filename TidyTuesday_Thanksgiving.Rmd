---
title: "Tidy_Tuesday_Thanksgiving"
author: "Andrew Borozan"
date: "`r Sys.Date()`"
output: html_document
---
This "Just Following Along" will track me following along to David Robinson's Tidy Tuesday for October 23, 2018. "Just Following Alongs" are simply that: me following someone else's workflow. The purpose is to learn from professionals; how they process and clean data, what visualizations they use, what packages they employ, what statistical methods they frequently apply, so on and so forth. I am not teaching in these "Just Follow Alongs" - I am simply following along and learning tricks of the trade. Think of these as me "showing my notes" after listening to a teacher's lecture. I add some flourishes here and there (I like making the graphs look more appealing), but most of the code comes from the tutorial.

I am the apprentice at the feet of the masters...

Here is video of David Robinson dissecting the Tidy Tuesday data:



<iframe width="900" height="506" src="https://www.youtube.com/embed/3-DRwg9yeNA" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>


<br>


First, I load in a bunch of packages, just in case...

```{r Loading Packages, message=FALSE, warning=FALSE}
library(pacman)
p_load(tidyverse, skimr, scales, rmarkdown, magrittr, lubridate, janitor, htmltab, ggrepel, viridis,
       ggthemes, knitr, rvest, reactable, RSelenium, stringr.plus, htmltools)
options(scipen=999)
```

Then we load in the data...

```{r Load in the Data, message=FALSE, warning=FALSE}
thanks <- read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2018/2018-11-20/thanksgiving_meals.csv")

view(thanks)
```



```{r Just looking at the data}
thanks %>% count(celebrate)

# Breakdown of respondents by age and gender
thanks %>% ggplot(aes(age, fill = gender)) +
  geom_bar()


# Do people pray?
thanks %>% count(prayer)

# Work on black friday?
thanks %>% count(black_friday)

# Community type?
thanks %>% count(community_type)

# Main dish?
thanks %>% count(main_dish, sort = TRUE)

# How prepared?
thanks %>% count(main_dish, main_prep, sort = TRUE)

# Cranberry?
thanks %>% count(cranberry)

# Family income?
thanks %>% count(family_income)

# The last one needs to be an ordered factor
thanks %>%
  mutate(family_income = fct_reorder(family_income, parse_number(family_income))) %>% count(family_income)
# factor reorder by parse_number(family income) - parse_number helpful in ordering numbers (works with $ and ,s) - sorts in the order we want

# Put that cleaning step in data set:
thanks <- thanks %>%
  mutate(family_income = fct_reorder(family_income, parse_number(family_income)))


```

Which of these fields influence canned or homemade cranberries? Richer the household have more homemade?
```{r}
thanks %>% group_by(family_income) %>% 
  filter(cranberry %in% c("Canned", "Homemade")) %>% 
  group_by(family_income) %>% 
  summarize(homemade = mean(cranberry == "Homemade"), 
            size = n()) %>% 
  ggplot(aes(family_income, homemade, group = 1)) +
  geom_line()+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Dave's little trick for confidence intervals
thanks %>% group_by(family_income) %>% 
  filter(cranberry %in% c("Canned", "Homemade")) %>% 
  group_by(family_income) %>% 
  summarize(homemade = sum(cranberry == "Homemade"), 
            total = n(), 
            low = qbeta(.025, homemade + .5, total - homemade +.5), 
            high = qbeta(.975, homemade +.5, total - homemade +.5)) %>% #This adds the confidence interval for the mean
  ggplot(aes(family_income, homemade / total, group = 1)) +
  geom_line()+
  geom_ribbon(aes(ymin = low, ymax = high), alpha = .2) +
  scale_y_continuous(labels = scales::percent_format())+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

#Jeffery's interval - quantiles of the Beta distribution - Little beyond my understanding. He didn't do regular confidence intervals, he did Jeffery's interval.


```



How many people celebrate Thanksgiving by family income?
```{r}

# Dave's little trick for confidence intervals
thanks %>% group_by(family_income)  %>% 
  group_by(family_income) %>% 
  summarize(celebrate = sum(celebrate == "Yes"), 
            total = n(), 
            low = qbeta(.025, celebrate + .5, total - celebrate +.5), 
            high = qbeta(.975, celebrate +.5, total - celebrate +.5)) %>% #This adds the confidence interval for the mean
  ggplot(aes(family_income, celebrate / total, group = 1)) +
  geom_line()+
  geom_ribbon(aes(ymin = low, ymax = high), alpha = .2) +
  scale_y_continuous(labels = scales::percent_format())+
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

Looking at side dishes. What are the most common sides, pies, and desserts?

```{r}
thanks %>% select(id, 
                  starts_with("pie"), 
                  starts_with("dessert"), 
                  starts_with("side"))

# To put all in one column use gather
food_gathered <- thanks %>% select(id, 
                  starts_with("pie"), 
                  starts_with("dessert"), 
                  starts_with("side")) %>% 
  select(-pie13, -side15, -dessert12) %>% 
  gather(type, value, -id) %>% # What this does is create a new column (type) and the values are the column names (ex. pie1), takes the value of the specified columns (everything but id), and places those values into the "value" column
 filter(!is.na(value),
        # !value %in% c("None", "Other (please specifiy)")) %>% 
        !grepl("None", x = value),
        !grepl("Other", x = value)) %>% #Either works, I like grepl better
  mutate(type = str_remove(type, "\\d+")) #regex removes numbers (ex. removes 1 from pie1)

theme_set(theme_classic())
food_gathered %>% count(type, value, sort = TRUE) %>% 
  mutate(value = fct_reorder(value, n)) %>% 
  ggplot(aes(value, n, fill = type)) +
  geom_col() +
  coord_flip()+
  scale_fill_viridis(option = "mako", discrete = TRUE)

n_respondents <- n_distinct(food_gathered$id)

n_respondents
# Dave was talking about getting the percentage of people that respond as opposed to raw number here. I was a little confused as to why he wants to count only the distinct responses....
 

food_gathered %>% count(type, value, sort = TRUE) %>% 
  mutate(value = fct_reorder(value, n)) %>% 
  ggplot(aes(value, n/n_respondents, fill = type)) +
  geom_col(show.legend = FALSE) +
  coord_flip()+
  scale_fill_viridis(option = "mako", discrete = TRUE)+
  scale_y_continuous(labels = scales::percent_format())+
  facet_wrap(~ type, scale = "free", ncol = 1)


```




How does taste/servings look by age?
```{r}
food_gathered %>% inner_join(thanks, by = "id") %>% 
  mutate(age_number = parse_number(age)) %>% #Gets age floors as number
  group_by(value) %>% 
  summarize(average_age = mean(age_number, na.rm = TRUE), 
            total = n()) %>% 
  arrange(desc(average_age))
```

Does region influence type of food served?
```{r}
food_by_region <- food_gathered %>% inner_join(thanks, by = "id") %>% 
  group_by(us_region) %>% 
  mutate(respondents = n_distinct(id)) %>% 
  count(us_region, respondents, type, value) %>% 
  ungroup() %>% 
  mutate(percent = n/respondents)

food_by_region %>% filter(value == "Apple") %>% 
  arrange(desc(percent))

food_by_region %>% filter(value == "Pumpkin") %>% 
  arrange(desc(percent))

food_by_region %>% filter(value == "Pecan") %>% 
  arrange(desc(percent))

food_by_region %>% filter(value == "Cornbread") %>% 
  arrange(desc(percent))
```


Types of food by prayer
```{r}
food_gathered %>% inner_join(thanks, by = "id") %>% 
  filter(!is.na(prayer)) %>% 
  group_by(type, value) %>% 
  summarize(prayer = sum(prayer == "Yes"), total = n(), 
            percent = prayer/total) %>% 
  arrange(desc(percent))
```


What sides, pies, and desserts are eaten together?
```{r}
food_gathered

library(widyr)

food_gathered %>% pairwise_cor(value, id, sort = TRUE) %>% 
  filter(item1 == "Pecan")

library(ggraph)
library(igraph)
food_cor <- food_gathered %>% pairwise_cor(value, id, sort = TRUE) %>% 
  head(50)

food_cor %>% graph_from_data_frame() %>% 
  ggraph() +
  geom_edge_link()+
  geom_node_point()


#Following along but got lost in the graphs. Graphs not easy to interpret
```



